---
title: "Websockets"
icon: "comments"
---

This API provides real-time [text-to-speech](https://elevenlabs.io/text-to-speech) conversion using WebSockets. This allows you to send a text message and receive audio data back in real-time.

<Note>
Endpoint:<br/>`wss://api.elevenlabs.io/v1/text-to-speech/{voice_id}/stream-input?model_id={model}`
</Note>

# When to use

The Text-to-Speech Websockets API is designed to generate audio from partial text input while ensuring consistency throughout the generated audio. Although highly flexible, the Websockets API isn't a one-size-fits-all solution. It's well-suited for scenarios where:
* The input text is being streamed or generated in chunks.
* Word-to-audio alignment information is required.

For a practical demonstration in a real world application, refer to the [Example of voice streaming using ElevenLabs and ChatGPT](#example-of-voice-streaming-using-elevenlabs-and-chatgpt) section.

# When not to use

However, it may not be the best choice when:
* The entire input text is available upfront. Given that the generations are partial, some buffering is involved, which could potentially result in slightly higher latency compared to a standard HTTP request.
* You want to quickly experiment or prototype. Working with Websockets can be harder and more complex than using a standard HTTP API, which might slow down rapid development and testing.

In these cases, use the [Text to Speech API](/api-reference/text-to-speech) instead.

# Protocol

The WebSocket API uses a bidirectional protocol that encodes all messages as JSON objects.

## Streaming input text

The client can send messages with text input to the server. The messages can contain the following fields:
```json
{
  "text": "This is a sample text ",
  "voice_settings": {
    "stability": 0.8,
    "similarity_boost": 0.8
  },
  "generation_config": {
    "chunk_length_schedule": [120, 160, 250, 290]
  },
  "xi_api_key": "<XI API Key>",
  "authorization": "Bearer <Authorization Token>"
}

```

<ParamField body="text" type="string" required>
    Should always end with a single space string `" "`. In the first message, the text should be a space `" "`.
</ParamField>

<ParamField body="try_trigger_generation" type="boolean" default={false} required>
    Use this to attempt to immediately trigger the generation of audio. Results may vary, as we determine the best time to generate audio based on various factors.
    
    This is useful when you want to immediately generate a response.
    
    Note that small amounts of text may result in lower quality audio, therefore, only use if you really need text to be processed immediately. We recommend adjusting the `chunk_length_schedule` in the `generation_config` instead.
</ParamField>

<ParamField body="voice_settings" type="object" >
  This property should only be provided in the first message you send.
  <Expandable title="properties">
      <ParamField body="stability" type="number" >
        Defines the stability for voice settings.
      </ParamField>

      <ParamField body="similarity_boost" type="number">
        Defines the similarity boost for voice settings.
      </ParamField>

      <ParamField body="style" type="number">
        Defines the style for voice settings. This parameter is available on V2+ models.
      </ParamField>

      <ParamField body="use_speaker_boost" type="boolean">
        Defines the use speaker boost for voice settings. This parameter is available on V2+ models.
      </ParamField>
  </Expandable>
</ParamField>

<ParamField body="generation_config" type="object">
   This property should only be provided in the first message you send.
  <Expandable title="properties" defaultOpen>

    <ParamField body="chunk_length_schedule" type="array">
        Determines the minimum amount of text that needs to be sent before the chunks start getting processed.

        Default is: [120, 160, 250, 290].

        This means that the first audio chunk will not be generated until you send text that is at least 120 characters long. The next chunk of audio will only be sent once a further 160 characters have been sent. The third audio chunk will be generated after the next 250 characters. Then the fourth and beyond, will be generated in sets of 290 characters.

        Customise this array to suit your needs. If you want to generate audio more frequently, you can reduce the values in the array. Note that setting the values too low may result in lower quality audio. Please test and adjust as needed.

        Each item should be in the range `50`-`500`.
    </ParamField>

  </Expandable>
</ParamField>

<ParamField body="flush" type="boolean">
    Flush forces the generation of audio. Set this value to `true` when you have finished sending text, but want to keep the websocket connection open.
    
    This is useful when you want to ensure that the last chunk of audio is generated even when the length of text sent is smaller than the value set in `chunk_length_schedule` (e.g. 120 or 50).
</ParamField>

<ParamField body="xi_api_key" type="string">
    Provide the XI API Key in the first message if it's not in the header. See [Authentication](/api-reference/quick-start/authentication) for more details.
</ParamField>

<ParamField body="authorization" type="string">
    Authorization bearer token. Should be provided only in the first message if not present in the header and the XI API Key is not provided.
</ParamField>

<Note>
For best latency we recommend streaming word-by-word, this way we will start generating as soon as we reach the predefined number of un-generated characters.
</Note>

##  Close connection

In order to close the connection, the client should send an End of Sequence (EOS) message. The EOS message should always be an empty string:


```json End of Sequence (EOS) message
{
    "text": ""
}
```

<ParamField body="text" type="string" required>
    Should always be an empty string `""`.
</ParamField>



## Streaming output audio

The server will always respond with a message containing the following fields:

```json Response message
{
    "audio": "Y3VyaW91cyBtaW5kcyB0aGluayBhbGlrZSA6KQ==",
    "isFinal": false,
    "normalizedAlignment": {
        "char_start_times_ms": [0, 3, 7, 9, 11, 12, 13, 15, 17, 19, 21],
        "chars_durations_ms": [3, 4, 2, 2, 1, 1, 2, 2, 2, 2, 3]
        "chars": ["H", "e", "l", "l", "o", " ", "w", "o", "r", "l", "d"]
    },
    "alignment": {
        "char_start_times_ms": [0, 3, 7, 9, 11, 12, 13, 15, 17, 19, 21],
        "chars_durations_ms": [3, 4, 2, 2, 1, 1, 2, 2, 2, 2, 3]
        "chars": ["H", "e", "l", "l", "o", " ", "w", "o", "r", "l", "d"]
    }
}
```

<ResponseField name="audio" type="string" optional="true">
  A generated partial audio chunk, encoded using the selected output_format, by default this is MP3 encoded as a base64 string.
</ResponseField>

<ResponseField name="isFinal" type="boolean" optional="true">
  Indicates if the generation is complete. If set to `True`, `audio` will be null.
</ResponseField>

<ResponseField name="normalizedAlignment" type="string" optional="true">
  Alignment information for the generated audio given the input normalized text sequence.
  <Expandable title="properties">

  <ParamField body="char_start_times_ms" type="array">
    A list of starting times (in milliseconds) for each character in the normalized text as it corresponds to the audio. For instance, the character 'H' starts at time 0 ms in the audio.
  </ParamField>

  <ParamField body="chars_durations_ms" type="array">
    A list providing the duration (in milliseconds) for each character's pronunciation in the audio. For instance, the character 'H' has a pronunciation duration of 3 ms.
  </ParamField>

  <ParamField body="chars" type="array">
    The list of characters in the normalized text sequence that corresponds with the timings and durations. This list is used to map the characters to their respective starting times and durations.
  </ParamField>
  </Expandable>
</ResponseField>

<ResponseField name="alignment" type="string" optional="true">
  Alignment information for the generated audio given the original text sequence.
  <Expandable title="properties">

  <ParamField body="char_start_times_ms" type="array">
    A list of starting times (in milliseconds) for each character in the original text as it corresponds to the audio. For instance, the character 'H' starts at time 0 ms in the audio.
  </ParamField>

  <ParamField body="chars_durations_ms" type="array">
    A list providing the duration (in milliseconds) for each character's pronunciation in the audio. For instance, the character 'H' has a pronunciation duration of 3 ms.
  </ParamField>

  <ParamField body="chars" type="array">
    The list of characters in the original text sequence that corresponds with the timings and durations. This list is used to map the characters to their respective starting times and durations.
  </ParamField>
  </Expandable>
</ResponseField>

## Path parameters

<ParamField path="voice_id" type="string">
    Voice ID to be used, you can use [Get Voices](/api-reference/voices) to list all the available voices.
</ParamField>

## Query parameters

<ParamField query="model_id" type="string">
    Identifier of the model that will be used, you can query them using [Get Voices](/api-reference/models-get).
</ParamField>
<ParamField query="enable_logging" type="string">
    Whether to enable request logging, if disabled the request will not be present in history nor bigtable.
    Enabled by default. Note: simple logging (aka printing) to stdout/stderr is always enabled.
</ParamField>
<ParamField query="enable_ssml_parsing" type="boolean">
    Whether to enable/disable parsing of SSML tags within the provided text. For best results, we recommend
    sending SSML tags as fully contained messages to the websockets endpoint, otherwise this may result in additional latency.
    Please note that rendered text, in normalizedAlignment, will be altered in support of SSML tags. The 
    rendered text will use a . as a placeholder for breaks, and syllables will be reported using the CMU arpabet alphabet where SSML phoneme tags are used to specify pronunciation.
    Disabled by default.
</ParamField>
<ParamField query="optimize_streaming_latency" type="string">
    You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values:
| Value | Description |
| --- | --- |
| 0 | default mode (no latency optimizations) |
| 1 | normal latency optimizations (about 50% of possible latency improvement of option 3) |
| 2 | strong latency optimizations (about 75% of possible latency improvement of option 3) |
| 3 | max latency optimizations |
| 4 | max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates). |

Defaults to `0`
</ParamField>
<ParamField query="output_format" type="string">
    Output format of the generated audio. Must be one of:
| Value | Description |
| --- | --- |
| mp3_44100 | default output format, mp3 with 44.1kHz sample rate |
| pcm_16000 | PCM format (S16LE) with 16kHz sample rate |
| pcm_22050 | PCM format (S16LE) with 22.05kHz sample rate |
| pcm_24000 | PCM format (S16LE) with 24kHz sample rate |
| pcm_44100 | PCM format (S16LE) with 44.1kHz sample rate |
| ulaw_8000 | μ-law format (mulaw) with 8kHz sample rate. (Note that this format is commonly used for Twilio audio inputs.) |

Defaults to `mp3_44100`
</ParamField>

# Example of voice streaming using ElevenLabs and OpenAI

The following example demonstrates how to leverage the ElevenLabs Websockets API to stream input from OpenAI's GPT model, while the answer is being generated, thereby minimizing the overall latency of the operation.

```python
import asyncio
import websockets
import json
import base64
import shutil
import os
import subprocess
from openai import AsyncOpenAI

# Define API keys and voice ID
OPENAI_API_KEY = '<OPENAI_API_KEY>'
ELEVENLABS_API_KEY = '<ELEVENLABS_API_KEY>'
VOICE_ID = '21m00Tcm4TlvDq8ikWAM'

# Set OpenAI API key
aclient = AsyncOpenAI(api_key=OPENAI_API_KEY)

def is_installed(lib_name):
    return shutil.which(lib_name) is not None


async def text_chunker(chunks):
    """Split text into chunks, ensuring to not break sentences."""
    splitters = (".", ",", "?", "!", ";", ":", "—", "-", "(", ")", "[", "]", "}", " ")
    buffer = ""

    async for text in chunks:
        if buffer.endswith(splitters):
            yield buffer + " "
            buffer = text
        elif text.startswith(splitters):
            yield buffer + text[0] + " "
            buffer = text[1:]
        else:
            buffer += text

    if buffer:
        yield buffer + " "


async def stream(audio_stream):
    """Stream audio data using mpv player."""
    if not is_installed("mpv"):
        raise ValueError(
            "mpv not found, necessary to stream audio. "
            "Install instructions: https://mpv.io/installation/"
        )

    mpv_process = subprocess.Popen(
        ["mpv", "--no-cache", "--no-terminal", "--", "fd://0"],
        stdin=subprocess.PIPE, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL,
    )

    print("Started streaming audio")
    async for chunk in audio_stream:
        if chunk:
            mpv_process.stdin.write(chunk)
            mpv_process.stdin.flush()

    if mpv_process.stdin:
        mpv_process.stdin.close()
    mpv_process.wait()


async def text_to_speech_input_streaming(voice_id, text_iterator):
    """Send text to ElevenLabs API and stream the returned audio."""
    uri = f"wss://api.elevenlabs.io/v1/text-to-speech/{voice_id}/stream-input?model_id=eleven_monolingual_v1"

    async with websockets.connect(uri) as websocket:
        await websocket.send(json.dumps({
            "text": " ",
            "voice_settings": {"stability": 0.5, "similarity_boost": 0.8},
            "xi_api_key": ELEVENLABS_API_KEY,
        }))

        async def listen():
            """Listen to the websocket for audio data and stream it."""
            while True:
                try:
                    message = await websocket.recv()
                    data = json.loads(message)
                    if data.get("audio"):
                        yield base64.b64decode(data["audio"])
                    elif data.get('isFinal'):
                        break
                except websockets.exceptions.ConnectionClosed:
                    print("Connection closed")
                    break

        listen_task = asyncio.create_task(stream(listen()))

        async for text in text_chunker(text_iterator):
            await websocket.send(json.dumps({"text": text, "try_trigger_generation": True}))

        await websocket.send(json.dumps({"text": ""}))

        await listen_task


async def chat_completion(query):
    """Retrieve text from OpenAI and pass it to the text-to-speech function."""
    response = await aclient.chat.completions.create(model='gpt-4', messages=[{'role': 'user', 'content': query}],
    temperature=1, stream=True)

    async def text_iterator():
        async for chunk in response:
            delta = chunk.choices[0].delta
            yield delta.content

    await text_to_speech_input_streaming(VOICE_ID, text_iterator())


# Main execution
if __name__ == "__main__":
    user_query = "Hello, tell me a very long story."
    asyncio.run(chat_completion(user_query))


```

# Other examples

Some examples for interacting with the Websocket API in different ways are provided below

<CodeGroup>

```python Python websockets and asyncio
import asyncio
import websockets
import json
import base64

async def text_to_speech(voice_id):
    model = 'eleven_monolingual_v1'
    uri = f"wss://api.elevenlabs.io/v1/text-to-speech/{voice_id}/stream-input?model_id={model}"

    async with websockets.connect(uri) as websocket:

        # Initialize the connection
        bos_message = {
            "text": " ",
            "voice_settings": {
                "stability": 0.5,
                "similarity_boost": 0.8
            },
            "xi_api_key": "api_key_here",  # Replace with your API key
        }
        await websocket.send(json.dumps(bos_message))

        # Send "Hello World" input
        input_message = {
            "text": "Hello World ",
            "try_trigger_generation": True
        }
        await websocket.send(json.dumps(input_message))

        # Send EOS message with an empty string instead of a single space
        # as mentioned in the documentation
        eos_message = {
            "text": ""
        }
        await websocket.send(json.dumps(eos_message))

        # Added a loop to handle server responses and print the data received
        while True:
            try:
                response = await websocket.recv()
                data = json.loads(response)
                print("Server response:", data)

                if data["audio"]:
                    chunk = base64.b64decode(data["audio"])
                    print("Received audio chunk")
                else:
                    print("No audio data in the response")
                    break
            except websockets.exceptions.ConnectionClosed:
                print("Connection closed")
                break

asyncio.get_event_loop().run_until_complete(text_to_speech("voice_id_here"))
```

```javascript Javascript websockets
const voiceId = "voice_id_here"; // replace with your voice_id
const model = 'eleven_monolingual_v1';
const wsUrl = `wss://api.elevenlabs.io/v1/text-to-speech/${voiceId}/stream-input?model_id=${model}`;
const socket = new WebSocket(wsUrl);

// 2. Initialize the connection by sending the BOS message
socket.onopen = function (event) {
    const bosMessage = {
        "text": " ",
        "voice_settings": {
            "stability": 0.5,
            "similarity_boost": 0.8
        },
        "xi_api_key": "api_key_here", // replace with your API key
    };

    socket.send(JSON.stringify(bosMessage));

    // 3. Send the input text message ("Hello World")
    const textMessage = {
        "text": "Hello World ",
        "try_trigger_generation": true,
    };

    socket.send(JSON.stringify(textMessage));

    // 4. Send the EOS message with an empty string
    const eosMessage = {
        "text": ""
    };

    socket.send(JSON.stringify(eosMessage));
};

// 5. Handle server responses
socket.onmessage = function (event) {
    const response = JSON.parse(event.data);

    console.log("Server response:", response);

    if (response.audio) {
        // decode and handle the audio data (e.g., play it)
        const audioChunk = atob(response.audio);  // decode base64
        console.log("Received audio chunk");
    } else {
        console.log("No audio data in the response");
    }

    if (response.isFinal) {
        // the generation is complete
    }

    if (response.normalizedAlignment) {
        // use the alignment info if needed
    }
};

// Handle errors
socket.onerror = function (error) {
    console.error(`WebSocket Error: ${error}`);
};

// Handle socket closing
socket.onclose = function (event) {
    if (event.wasClean) {
        console.info(`Connection closed cleanly, code=${event.code}, reason=${event.reason}`);
    } else {
        console.warn('Connection died');
    }
};
```

```python elevenlabs-python
from elevenlabs import generate, stream

def text_stream():
    yield "Hi there, I'm Eleven "
    yield "I'm a text to speech API "

audio_stream = generate(
    text=text_stream(),
    voice="Nicole",
    model="eleven_monolingual_v1",
    stream=True
)

stream(audio_stream)
```
</CodeGroup>
