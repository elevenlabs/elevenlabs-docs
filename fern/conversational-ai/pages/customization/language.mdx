---
title: Language
subtitle: Learn how to configure your agent to speak multiple languages.
---

## Overview

This guide shows you how to configure your agent to speak multiple languages. You'll learn to:

- Configure your agent's primary language
- Add support for multiple languages
- Set language-specific voices and first messages
- Optimize voice selection for natural pronunciation

## Guide

<Steps>

<Step title="Default agent language">
When you create a new agent, it's configured with:

- English as the primary language
- Flash v2 model for fast, English-only responses
- A default first message.

<Frame background="subtle">![](/assets/images/conversational-ai/language-overview.png)</Frame>

<Note>
  Additional languages switch the agent to use the v2.5 Multilingual model. English will always use
  the v2 model.
</Note>

</Step>

<Step title="Add additional languages">
First, navigate to your agent's configuration page and locate the **Agent** tab.

1. In the **Additional Languages** add an additional language (e.g. French)
2. Review the first message, which is automatically translated using a Large Language Model (LLM). Customize it as needed for each additional language to ensure accuracy and cultural relevance.

<Frame background="subtle">![](/assets/images/conversational-ai/language-selection.png)</Frame>

<Note>
  Selecting the **All** option in the **Additional Languages** dropdown will configure the agent to
  support 31 languages. Collectively, these languages are spoken by approximately 90% of the world's
  population.
</Note>

</Step>

<Step title="Configure language-specific voices">
For optimal pronounciation, configure each additional language with a language-specific voice from our [Voice Library](https://elevenlabs.io/app/voice-library).

<Tabs>
<Tab title="Language-specific voice settings">
<Frame background="subtle">![](/assets/images/conversational-ai/language-voice.png)</Frame>
</Tab>
<Tab title="Voice library">
<Frame background="subtle">![](/assets/images/conversational-ai/voice-library-language.png)</Frame>
</Tab>

</Tabs>
</Step>

<Step title="Starting a call">

Now that the agent is configured to support additional languages, the widget will prompt the user for their preferred language before the conversation begins.

If using the SDK, the language can be set programmatically using conversation overrides. See the
[Overrides](/docs/conversational-ai/customization/dynamic-conversation) guide for implementation details.

<Frame background="subtle">![](/assets/images/conversational-ai/widget-language.png)</Frame>

<Note>
  Language selection is fixed for the duration of the call - users cannot switch languages
  mid-conversation.
</Note>

</Step>

</Steps>

### Internationalization

You can integrate the widget with your internationalization framework by dynamically setting the language and UI text attributes.

```html title="Widget"
<elevenlabs-convai
  language="es"
  action-text={i18n["es"]["actionText"]}
  start-call-text={i18n["es"]["startCall"]}
  end-call-text={i18n["es"]["endCall"]}
  expand-text={i18n["es"]["expand"]}
  listening-text={i18n["es"]["listening"]}
  speaking-text={i18n["es"]["speaking"]}
></elevenlabs-convai>
```

<Note>
  Ensure the language codes match between your i18n framework and the agent's supported languages.
</Note>

## Best practices

<AccordionGroup>
<Accordion title="Voice selection">
  Select voices specifically trained in your target languages. This ensures:
  - Natural pronunciation
  - Appropriate regional accents
  - Better handling of language-specific nuances
</Accordion>

<Accordion title="First message customization">
While automatic translations are provided, consider:

<div>
  
 - Reviewing translations for accuracy 
 - Adapting greetings for cultural context 
 - Adjusting formal/informal tone as needed

</div>
</Accordion>
</AccordionGroup>
