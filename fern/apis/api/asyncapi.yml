asyncapi: 2.6.0

info:
  title: ElevenLabs real-time API
  description: ElevenLabs real-time API
  license:
    name: MIT License
  version: 1.1.0

servers:
  API:
    url: wss://api.elevenlabs.io
    protocol: wss
    description: AssemblyAI API

tags:
  - name: textToSpeech

channels:
  /v1/text-to-speech/{voice_id}/stream-input:
    parameters:
      voice_id:
        description: The unique identifier for the voice to use in the TTS process.
        schema:
          type: string
    description: |
      The Text-to-Speech Websockets API is designed to generate audio from partial text input 
      while ensuring consistency throughout the generated audio. Although highly flexible, 
      the Websockets API isn't a one-size-fits-all solution. It's well-suited for scenarios where:
        * The input text is being streamed or generated in chunks.
        * Word-to-audio alignment information is required.

      However, it may not be the best choice when:
        * The entire input text is available upfront. Given that the generations are partial, 
          some buffering is involved, which could potentially result in slightly higher latency compared 
          to a standard HTTP request.
        * You want to quickly experiment or prototype. Working with Websockets can be harder and more 
          complex than using a standard HTTP API, which might slow down rapid development and testing.
    bindings:
      ws:
        query:
          type: object
          properties:
            model_id:
              description: The model ID to use
              type: string
            language_code:
              description: The ISO 639-1 language code (for Turbo v2.5 and Flash v2.5 models only)
              type: string
            enable_logging:
              description: Whether to enable logging of the request
              type: string
            enable_ssml_parsing:
              description: Whether to enable SSML parsing
              type: boolean
              default: false
            optimize_streaming_latency:
              description: Latency optimization level (deprecated)
              type: string
              enum: ['0', '1', '2', '3', '4']
              default: '0'
            output_format:
              description: The output audio format
              type: string
              enum: ['mp3_44100', 'pcm_16000', 'pcm_22050', 'pcm_24000', 'pcm_44100', 'ulaw_8000']
              default: 'mp3_44100'
            inactivity_timeout:
              description: Timeout for inactivity before connection is closed
              type: number
              default: 20
            sync_alignment:
              description: Whether to include timing data with every audio chunk
              type: boolean
              default: false
            auto_mode:
              description: This parameter focuses on reducing the latency by disabling the chunk schedule and all buffers. It is only recommended when sending full sentences or phrases, sending partial phrases will result in highly reduced quality. By default it's set to false.
              type: boolean
              default: false
    publish:
      description: Send messages to the WebSocket
      operationId: sendMessage
      message:
        oneOf:
          - $ref: '#/components/messages/InitializeConnection'
          - $ref: '#/components/messages/SendText'
          - $ref: '#/components/messages/CloseConnection'
    subscribe:
      description: Receive messages from the WebSocket
      operationId: receiveMessage
      message:
        oneOf:
          - $ref: '#/components/messages/AudioOutput'
          - $ref: '#/components/messages/FinalOutput'
    x-fern-examples:
      - query-parameters:
          model_id: pcm_s16le
        messages:
          - type: publish
            messageId: InitializeConnection
            value:
              text: ' '
              voice_settings:
                stability: 0.5
                similarity_boost: 0.8
              xi_api_key: <YOUR_API_KEY>
          - type: publish
            messageId: SendText
            value:
              text: 'Hello World'
              try_trigger_generation: true
          - type: publish
            messageId: CloseConnection
            value:
              text: ''
          - type: subscribe
            messageId: AudioOutput
            value:
              audio: Y3VyaW91cyBtaW5kcyB0aGluayBhbGlrZSA6KQ==
components:
  messages:
    SendText:
      messageId: sendText
      payload:
        $ref: '#/components/schemas/SendText'
    InitializeConnection:
      messageId: initializeConnection
      payload:
        $ref: '#/components/schemas/InitializeConnection'
    CloseConnection:
      messageId: closeConnection
      payload:
        $ref: '#/components/schemas/CloseConnection'
    AudioOutput:
      messageId: audioOutput
      payload:
        $ref: '#/components/schemas/AudioOutput'
    FinalOutput:
      messageId: finalOutput
      payload:
        $ref: '#/components/schemas/FinalOutput'
  schemas:
    InitializeConnection:
      type: object
      properties:
        text:
          x-fern-type: literal<" ">
          description: The initial text that must be sent is a blank space.
        voice_settings:
          $ref: '#/components/schemas/RealtimeVoiceSettings'
        generation_config:
          $ref: '#/components/schemas/GenerationConfig'
          description: 'This property should only be provided in the first message you send. '
        xi-api-key:
          type: string
          description: |
            Your ElevenLabs API key. This is a required parameter that should be provided in the first message you send. 
            You can find your API key in the [API Keys section](https://elevenlabs.io/docs/api-reference/websockets#api-keys).
      required:
        - text
        - xi-api-key

    CloseConnection:
      type: object
      properties:
        text:
          x-fern-type: literal<"">
          description: End the stream with an empty string
      required:
        - text

    SendText:
      type: object
      required:
        - text
      properties:
        text:
          type: string
          description: The text to be sent to the API for audio generation. SHould always end with a single space string.
        try_trigger_generation:
          description: |
            This is an advanced setting that most users shouldn't need to use. It relates to our generation schedule 
            explained [here](#understanding-how-our-websockets-buffer-text).

            Use this to attempt to immediately trigger the generation of audio, overriding the `chunk_length_schedule`. 
            Unlike flush, `try_trigger_generation` will only generate audio if our 
            buffer contains more than a minimum 
            threshold of characters, this is to ensure a higher quality response from our model. 

            Note that overriding the chunk schedule to generate small amounts of 
            text may result in lower quality audio, therefore, only use this parameter if you 
            really need text to be processed immediately. We generally recommend keeping the default value of 
            `false` and adjusting the `chunk_length_schedule` in the `generation_config` instead.
          type: boolean
        voice_settings:
          type: object
          properties:
            stability:
              type: number
              description: Defines the stability for voice settings.
            similarity_boost:
              type: number
              description: Defines the similarity boost for voice settings.
            style:
              type: number
              description: Defines the style for voice settings. Available on V2+ models.
            use_speaker_boost:
              type: boolean
              description: Defines whether to use speaker boost for voice settings. Available on V2+ models.
          required:
            - stability
            - similarity_boost
        generator_config:
          type: object
          properties:
            chunk_length_schedule:
              type: array
              items:
                type: number
              description: |
                This is an advanced setting that most users shouldn't need to use. It relates to our 
                generation schedule explained [here](https://elevenlabs.io/docs/api-reference/websockets#understanding-how-our-websockets-buffer-text).

                Determines the minimum amount of text that needs to be sent and present in our 
                buffer before audio starts being generated. This is to maximise the amount of context available to 
                the model to improve audio quality, whilst balancing latency of the returned audio chunks.

                The default value is: [120, 160, 250, 290].

                This means that the first chunk of audio will not be generated until you send text that 
                totals at least 120 characters long. The next chunk of audio will only be generated once a 
                further 160 characters have been sent. The third audio chunk will be generated after the 
                next 250 characters. Then the fourth, and beyond, will be generated in sets of at least 290 characters.

                Customize this array to suit your needs. If you want to generate audio more frequently 
                to optimise latency, you can reduce the values in the array. Note that setting the values 
                too low may result in lower quality audio. Please test and adjust as needed.

                Each item should be in the range 50-500.
        flush:
          type: boolean
          description: |
            Flush forces the generation of audio. Set this value to true when you have finished sending text, but want to keep the websocket connection open.

            This is useful when you want to ensure that the last chunk of audio is generated even when the length of text sent is smaller than the value set in chunk_length_schedule (e.g. 120 or 50).
    RealtimeVoiceSettings:
      properties:
        stability:
          type: number
          title: Stability
          description: Defines the stability for voice settings.
        similarity_boost:
          type: number
          title: Similarity Boost
          description: Defines the similarity boost for voice settings.
        style:
          type: number
          title: Style
          default: 0
          description: Defines the style for voice settings. This parameter is available on V2+ models.
        use_speaker_boost:
          type: boolean
          title: Use Speaker Boost
          description: Defines the use speaker boost for voice settings. This parameter is available on V2+ models.
      type: object
      required:
        - stability
        - similarity_boost

    GenerationConfig:
      properties:
        chunk_length_schedule:
          type: array
          items:
            type: number
          description: |
            This is an advanced setting that most users shouldn't need to use. It relates to our 
            generation schedule explained [here](https://elevenlabs.io/docs/api-reference/websockets#understanding-how-our-websockets-buffer-text).

            Determines the minimum amount of text that needs to be sent and present in our 
            buffer before audio starts being generated. This is to maximise the amount of context available to 
            the model to improve audio quality, whilst balancing latency of the returned audio chunks.

            The default value is: [120, 160, 250, 290].

            This means that the first chunk of audio will not be generated until you send text that 
            totals at least 120 characters long. The next chunk of audio will only be generated once a 
            further 160 characters have been sent. The third audio chunk will be generated after the 
            next 250 characters. Then the fourth, and beyond, will be generated in sets of at least 290 characters.

            Customize this array to suit your needs. If you want to generate audio more frequently 
            to optimise latency, you can reduce the values in the array. Note that setting the values 
            too low may result in lower quality audio. Please test and adjust as needed.

            Each item should be in the range 50-500.

    AudioOutput:
      type: object
      required:
        - audio
      properties:
        audio:
          type: string
          # format: binary
          description: |
            A generated partial audio chunk, encoded using the selected output_format, by default this 
            is MP3 encoded as a base64 string.

    FinalOutput:
      type: object
      properties:
        isFinal:
          x-fern-type: literal<true>
          description: |
            Indicates if the generation is complete. If set to `True`, `audio` will be null.
        normalizedAlignment:
          $ref: '#/components/schemas/NormalizedAlignment'
        alignment:
          $ref: '#/components/schemas/Alignment'

    NormalizedAlignment:
      type: object
      description: |
        Alignment information for the generated audio given the input normalized text sequence.
      properties:
        char_start_times_ms:
          x-fern-type: list<integer>
          description: |
            A list of starting times (in milliseconds) for each character in the normalized text as it 
            corresponds to the audio. For instance, the character 'H' starts at time 0 ms in the audio.  
            Note these times are relative to the returned chunk from the model, and not the 
            full audio response.
        chars_durations_ms:
          x-fern-type: list<integer>
          description: |
            A list of durations (in milliseconds) for each character in the normalized text as it 
            corresponds to the audio. For instance, the character 'H' lasts for 3 ms in the audio.  
            Note these times are relative to the returned chunk from the model, and not the 
            full audio response.
        chars:
          x-fern-type: list<string>
          description: |
            A list of characters in the normalized text sequence. For instance, the first character is 'H'. 
            Note that this list may contain spaces, punctuation, and other special characters. 
            The length of this list should be the same as the lengths of `char_start_times_ms` and `chars_durations_ms`.

    Alignment:
      type: object
      description: |
        Alignment information for the generated audio given the input text sequence.
      properties:
        char_start_times_ms:
          x-fern-type: list<integer>
          description: |
            A list of starting times (in milliseconds) for each character in the text as it 
            corresponds to the audio. For instance, the character 'H' starts at time 0 ms in the audio.  
            Note these times are relative to the returned chunk from the model, and not the 
            full audio response.
        chars_durations_ms:
          x-fern-type: list<integer>
          description: |
            A list of durations (in milliseconds) for each character in the text as it 
            corresponds to the audio. For instance, the character 'H' lasts for 3 ms in the audio.  
            Note these times are relative to the returned chunk from the model, and not the 
            full audio response.
        chars:
          x-fern-type: list<string>
          description: |
            A list of characters in the text sequence. For instance, the first character is 'H'. 
            Note that this list may contain spaces, punctuation, and other special characters. 
            The length of this list should be the same as the lengths of `char_start_times_ms` and `chars_durations_ms`.
